{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-So2CGsZXwIU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv(\"/content/dialogs_expanded.csv\", encoding='latin1')\n",
        "\n",
        "# Preprocessing\n",
        "train_data = train_data.dropna()  # Remove rows with missing values\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Preprocess the text\n",
        "train_data['question'] = train_data['question'].apply(preprocess_text)\n",
        "train_data['answer'] = train_data['answer'].apply(preprocess_text)\n",
        "\n",
        "# Extract the input (question) and target (answer) texts\n",
        "input_texts = train_data['question'].tolist()\n",
        "target_texts = train_data['answer'].tolist()\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_seq_length = max(max([len(seq) for seq in input_sequences]), max([len(seq) for seq in target_sequences]))\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_seq_length, padding='post')\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# The data is now ready to be used for training a seq2seq model with attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Train-test split\n",
        "input_train, input_val, target_train, target_val = train_test_split(input_sequences, target_sequences, test_size=0.2)\n",
        "\n",
        "target_train = np.array(target_train)\n",
        "target_val = np.array(target_val)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bm6YFp7-eng8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 128\n",
        "lstm_units = 128\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention Mechanism\n",
        "attention = Attention()  # Scaled Dot-Product Attention\n",
        "context_vector = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate context vector and decoder outputs\n",
        "decoder_concat_input = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
        "\n",
        "# Dense Layer\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "model.fit([input_train, input_train], target_train, batch_size=16, epochs=1, validation_data=([input_val, input_val], target_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "Dp4c3skKdW1N",
        "outputId": "50c6434e-50ad-49da-8ba8-6de34cdb1b37",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m  20/6971\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:03:43\u001b[0m 3s/step - accuracy: 0.5724 - loss: 9.5784"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-481da437cd57>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define parameters\n",
        "lstm_units = 256  # Example value\n",
        "embedding_dim = 128  # Example value\n",
        "vocab_size = 10000  # Example value\n",
        "max_seq_length = 30  # Example value\n",
        "\n",
        "# Define the embedding layer\n",
        "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# Define the embedding layer for encoder (for consistency)\n",
        "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# Encoder model\n",
        "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "encoder_embedded = encoder_embedding(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedded)\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder Inference Model\n",
        "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
        "decoder_state_input_h = Input(shape=(lstm_units,), name='decoder_state_input_h')\n",
        "decoder_state_input_c = Input(shape=(lstm_units,), name='decoder_state_input_c')\n",
        "decoder_hidden_state_input = Input(shape=(None, lstm_units), name='decoder_hidden_state_input')\n",
        "\n",
        "# Apply embedding layer to decoder inputs\n",
        "decoder_embedding_inf = decoder_embedding(decoder_inputs)\n",
        "\n",
        "# Define LSTM layer for the decoder\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
        "    decoder_embedding_inf, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        ")\n",
        "\n",
        "# Define Attention layer\n",
        "attention = Attention(name='attention')\n",
        "context_vector_inf = attention([decoder_outputs_inf, decoder_hidden_state_input])\n",
        "\n",
        "# Concatenate context vector and decoder outputs\n",
        "decoder_concat_input_inf = Concatenate(axis=-1)([context_vector_inf, decoder_outputs_inf])\n",
        "\n",
        "# Define Dense layer for the decoder outputs\n",
        "decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs_inf = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs_inf, state_h_inf, state_c_inf]\n",
        ")\n",
        "\n",
        "# Function to Decode Sequence Using Beam Search\n",
        "def beam_search_decode_sequence(input_seq, beam_width=3):\n",
        "    # Encode the input as state vectors\n",
        "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Check if 'starttoken' and 'endtoken' are in the tokenizer's vocabulary\n",
        "    start_token_index = tokenizer.word_index.get('starttoken', None)\n",
        "    end_token_index = tokenizer.word_index.get('endtoken', None)\n",
        "\n",
        "    if start_token_index is None or end_token_index is None:\n",
        "        raise ValueError(\"Start token or end token not found in tokenizer's word index\")\n",
        "\n",
        "    # Initialize beams\n",
        "    beams = [([start_token_index], 0.0)]  # (sequence, score)\n",
        "\n",
        "    while beams:\n",
        "        all_candidates = []\n",
        "        for seq, score in beams:\n",
        "            target_seq = np.array(seq).reshape(1, -1)\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq, encoder_outputs, state_h, state_c])\n",
        "            top_tokens = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n",
        "\n",
        "            for token in top_tokens:\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score - np.log(output_tokens[0, -1, token])\n",
        "                if token == end_token_index or len(new_seq) > max_seq_length:\n",
        "                    return ' '.join([tokenizer.index_word.get(t, '') for t in new_seq if t != start_token_index])\n",
        "\n",
        "                all_candidates.append((new_seq, new_score))\n",
        "\n",
        "        # Select the top beam_width sequences\n",
        "        beams = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "    return ''\n",
        "\n",
        "# Function to Preprocess Input Text\n",
        "def preprocess_input_text(input_text):\n",
        "    input_text = preprocess_text(input_text)\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_seq_length, padding='post')\n",
        "    return input_seq\n",
        "\n",
        "# Check the tokenizer for special tokens\n",
        "print(\"Tokenizer word index:\", tokenizer.word_index)\n",
        "\n",
        "# Ensure special tokens are added\n",
        "special_tokens = {\n",
        "    'starttoken': 1,\n",
        "    'endtoken': 2\n",
        "}\n",
        "\n",
        "tokenizer.word_index.update(special_tokens)\n",
        "tokenizer.index_word.update({v: k for k, v in special_tokens.items()})\n",
        "\n",
        "# Update vocab_size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# Example Usage\n",
        "input_text = \"upset?\"\n",
        "input_seq = preprocess_input_text(input_text)\n",
        "decoded_sentence = beam_search_decode_sequence(input_seq)\n",
        "\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Response: {decoded_sentence}\")\n"
      ],
      "metadata": {
        "id": "irQCmavjM3Jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3dda55c-ac55-47e7-f88d-43bbc9dc9ad9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer word index: {'i': 1, 'you': 2, 'the': 3, 'to': 4, 'a': 5, 'it': 6, 'that': 7, 'do': 8, 'what': 9, 'is': 10, 'of': 11, 'and': 12, 'have': 13, 'are': 14, 'in': 15, 'they': 16, 'was': 17, 'its': 18, 'did': 19, 'so': 20, 'like': 21, 'yes': 22, 'for': 23, 'my': 24, 'dont': 25, 'but': 26, 'he': 27, 'no': 28, 'be': 29, 'me': 30, 'im': 31, 'thats': 32, 'about': 33, 'we': 34, 'on': 35, 'how': 36, 'go': 37, 'not': 38, 'think': 39, 'too': 40, 'why': 41, 'your': 42, 'well': 43, 'going': 44, 'good': 45, 'will': 46, 'with': 47, 'want': 48, 'really': 49, 'get': 50, 'know': 51, 'all': 52, 'at': 53, 'there': 54, 'ill': 55, 'one': 56, 'just': 57, 'can': 58, 'this': 59, 'would': 60, 'if': 61, 'youre': 62, 'people': 63, 'see': 64, 'then': 65, 'she': 66, 'right': 67, 'nice': 68, 'didnt': 69, 'out': 70, 'should': 71, 'whats': 72, 'time': 73, 'need': 74, 'her': 75, 'money': 76, 'oh': 77, 'maybe': 78, 'him': 79, 'course': 80, 'lot': 81, 'when': 82, 'or': 83, 'much': 84, 'okay': 85, 'got': 86, 'up': 87, 'cant': 88, 'because': 89, 'sure': 90, 'day': 91, 'only': 92, 'school': 93, 'new': 94, 'an': 95, 'mean': 96, 'had': 97, 'them': 98, 'been': 99, 'ive': 100, 'make': 101, 'were': 102, 'who': 103, 'take': 104, 'great': 105, 'every': 106, 'look': 107, 'our': 108, 'some': 109, 'does': 110, 'love': 111, 'now': 112, 'something': 113, 'from': 114, 'say': 115, 'movie': 116, 'more': 117, 'next': 118, 'us': 119, 'give': 120, 'his': 121, 'today': 122, 'where': 123, 'never': 124, 'has': 125, 'better': 126, 'hes': 127, 'off': 128, 'tell': 129, 'hope': 130, 'any': 131, 'old': 132, 'doesnt': 133, 'job': 134, 'here': 135, 'two': 136, 'very': 137, 'come': 138, 'said': 139, 'their': 140, 'thank': 141, 'fun': 142, 'buy': 143, 'car': 144, 'sounds': 145, 'other': 146, 'eat': 147, 'put': 148, 'big': 149, 'weather': 150, 'anything': 151, 'house': 152, 'tv': 153, 'ever': 154, 'someone': 155, 'pretty': 156, 'even': 157, 'work': 158, 'long': 159, 'as': 160, 'food': 161, 'theyre': 162, 'bad': 163, 'havent': 164, 'always': 165, 'into': 166, 'many': 167, 'back': 168, 'party': 169, 'problem': 170, 'yeah': 171, 'still': 172, 'world': 173, 'doing': 174, 'than': 175, 'heard': 176, 'lets': 177, 'little': 178, 'idea': 179, 'went': 180, 'over': 181, 'nothing': 182, 'nose': 183, 'believe': 184, 'best': 185, 'use': 186, 'game': 187, 'feel': 188, 'told': 189, 'talk': 190, 'clean': 191, 'by': 192, 'might': 193, 'wish': 194, 'same': 195, 'tomorrow': 196, 'home': 197, 'news': 198, 'hard': 199, 'let': 200, 'after': 201, 'shes': 202, 'first': 203, 'happened': 204, 'years': 205, 'matter': 206, 'wont': 207, 'night': 208, 'isnt': 209, 'could': 210, 'different': 211, 'kind': 212, 'restaurant': 213, 'minutes': 214, 'hot': 215, 'wait': 216, 'am': 217, 'thought': 218, 'stop': 219, 'find': 220, 'down': 221, 'smell': 222, 'sorry': 223, 'yet': 224, 'bought': 225, 'guess': 226, 'show': 227, 'rain': 228, 'call': 229, 'five': 230, 'before': 231, 'else': 232, 'everyone': 233, 'most': 234, 'things': 235, 'phone': 236, 'thing': 237, 'already': 238, 'watch': 239, 'water': 240, 'lots': 241, 'talking': 242, 'those': 243, 'another': 244, 'last': 245, 'youll': 246, 'thanks': 247, 'wrong': 248, 'happy': 249, 'wasnt': 250, 'until': 251, 'gets': 252, 'wanted': 253, 'seen': 254, 'eyes': 255, 'these': 256, 'mine': 257, 'friday': 258, 'friend': 259, 'start': 260, 'hate': 261, 'dinner': 262, 'hear': 263, 'while': 264, 'close': 265, 'care': 266, 'cold': 267, 'weekend': 268, 'change': 269, 'around': 270, 'theres': 271, 'shouldnt': 272, 'yesterday': 273, 'cost': 274, 'three': 275, 'gave': 276, 'cars': 277, 'date': 278, 'police': 279, 'started': 280, 'says': 281, 'busy': 282, 'hurt': 283, 'man': 284, 'year': 285, 'enough': 286, 'must': 287, 'life': 288, 'looking': 289, 'woman': 290, 'play': 291, 'which': 292, 'id': 293, 'later': 294, 'looks': 295, 'took': 296, 'week': 297, 'saw': 298, 'help': 299, 'pounds': 300, 'left': 301, 'cheese': 302, 'save': 303, 'away': 304, 'doctor': 305, 'sometimes': 306, 'reason': 307, 'light': 308, 'again': 309, 'once': 310, 'movies': 311, 'keep': 312, 'family': 313, 'city': 314, 'forever': 315, 'bathroom': 316, 'cut': 317, 'dogs': 318, 'ticket': 319, 'kids': 320, 'real': 321, 'outside': 322, 'enjoy': 323, 'rains': 324, 'beach': 325, 'thinking': 326, 'probably': 327, 'sick': 328, 'class': 329, 'whole': 330, 'couldnt': 331, 'english': 332, 'glad': 333, 'having': 334, 'rich': 335, 'each': 336, 'dirty': 337, 'easy': 338, 'bet': 339, 'run': 340, 'perfect': 341, 'please': 342, 'feet': 343, 'favorite': 344, 'made': 345, 'everything': 346, 'friends': 347, 'street': 348, 'makes': 349, 'four': 350, 'vote': 351, 'especially': 352, 'cool': 353, 'way': 354, 'supposed': 355, 'times': 356, 'youve': 357, 'shoes': 358, 'couple': 359, 'listen': 360, 'lost': 361, 'catch': 362, 'try': 363, 'walk': 364, 'almost': 365, 'leave': 366, 'coffee': 367, 'fish': 368, 'mom': 369, 'used': 370, 'fat': 371, 'read': 372, '20': 373, 'room': 374, 'cigarette': 375, 'wouldnt': 376, 'soon': 377, 'true': 378, 'beautiful': 379, 'store': 380, 'found': 381, 'came': 382, 'free': 383, 'getting': 384, 'turn': 385, 'bread': 386, 'students': 387, 'travel': 388, 'may': 389, 'winter': 390, 'such': 391, 'understand': 392, 'stay': 393, 'girl': 394, 'ago': 395, 'music': 396, 'both': 397, 'baby': 398, 'god': 399, 'visit': 400, 'live': 401, 'dog': 402, 'dollar': 403, 'terrible': 404, 'deal': 405, 'cat': 406, 'costs': 407, 'black': 408, 'knows': 409, 'lose': 410, 'looked': 411, 'hour': 412, 'problems': 413, 'youd': 414, 'boy': 415, 'teach': 416, 'using': 417, 'story': 418, 'without': 419, 'hand': 420, 'white': 421, 'dangerous': 422, 'ball': 423, 'president': 424, 'high': 425, 'also': 426, 'neither': 427, 'pizza': 428, 'sunday': 429, 'means': 430, 'hell': 431, 'plane': 432, 'jokes': 433, 'check': 434, '50': 435, 'bring': 436, 'win': 437, 'butter': 438, 'orange': 439, 'lately': 440, 'far': 441, 'minute': 442, 'called': 443, 'through': 444, 'won': 445, 'end': 446, '8': 447, 'working': 448, 'invitation': 449, 'eight': 450, 'fast': 451, 'girlfriend': 452, 'goes': 453, 'asked': 454, 'price': 455, 'full': 456, 'blue': 457, 'late': 458, 'digital': 459, 'cigarettes': 460, 'waiter': 461, 'eating': 462, 'exactly': 463, 'california': 464, 'ask': 465, 'crazy': 466, 'starts': 467, 'ready': 468, 'divorced': 469, 'homework': 470, 'pick': 471, 'dad': 472, 'card': 473, 'forget': 474, 'remember': 475, 'wipe': 476, 'pants': 477, 'fix': 478, 'till': 479, 'college': 480, 'number': 481, 'tires': 482, 'hit': 483, 'baseball': 484, 'delicious': 485, 'table': 486, 'percent': 487, 'asking': 488, 'seems': 489, 'happen': 490, 'feeling': 491, 'earlier': 492, 'serious': 493, 'draw': 494, 'paint': 495, 'loved': 496, 'team': 497, 'tried': 498, 'waiting': 499, 'oclock': 500, 'pasadena': 501, 'likes': 502, 'schools': 503, 'everybody': 504, 'finish': 505, 'open': 506, 'grow': 507, 'paper': 508, 'fruit': 509, 'few': 510, 'numbers': 511, 'shopping': 512, 'front': 513, 'potatoes': 514, 'actually': 515, 'middle': 516, 'rather': 517, 'air': 518, 'fresh': 519, 'trying': 520, 'missing': 521, 'watching': 522, 'whatever': 523, 'hold': 524, 'wash': 525, 'guys': 526, 'lunch': 527, 'friendly': 528, 'parents': 529, 'father': 530, 'button': 531, 'wake': 532, 'blow': 533, 'men': 534, 'gives': 535, 'own': 536, '30': 537, 'afford': 538, 'pocket': 539, 'wonder': 540, 'sale': 541, 'half': 542, 'seats': 543, 'blind': 544, 'crime': 545, 'hire': 546, 'gun': 547, 'war': 548, 'laid': 549, 'power': 550, 'taxes': 551, 'face': 552, 'yourself': 553, 'raining': 554, 'sky': 555, 'trip': 556, 'tall': 557, 'promotion': 558, 'outfit': 559, 'art': 560, 'superbad': 561, 'funny': 562, 'either': 563, 'nobody': 564, 'question': 565, 'coming': 566, 'nosey': 567, 'polite': 568, 'being': 569, 'anymore': 570, 'bottom': 571, 'envelope': 572, 'stamp': 573, 'kitchen': 574, 'hungry': 575, 'rude': 576, 'die': 577, 'husband': 578, 'shirt': 579, 'radio': 580, 'comes': 581, 'tissue': 582, 'age': 583, 'worried': 584, 'carry': 585, 'pen': 586, 'smart': 587, 'flag': 588, 'pockets': 589, 'parking': 590, 'fire': 591, 'hours': 592, 'blinking': 593, 'tree': 594, 'singers': 595, 'drugs': 596, 'steak': 597, 'building': 598, 'break': 599, 'salt': 600, 'milk': 601, 'voted': 602, 'fine': 603, 'absolutely': 604, 'though': 605, 'mind': 606, 'telling': 607, 'under': 608, 'kinds': 609, 'dvd': 610, 'line': 611, 'type': 612, 'listening': 613, 'anyone': 614, 'myself': 615, 'given': 616, 'morning': 617, 'stand': 618, 'worry': 619, 'agree': 620, 'laptop': 621, 'loves': 622, 'animals': 623, 'helped': 624, 'mail': 625, 'puppy': 626, 'heaven': 627, 'died': 628, 'cents': 629, 'sheets': 630, 'drive': 631, 'loud': 632, 'point': 633, 'hole': 634, 'fell': 635, 'safe': 636, 'driver': 637, 'quit': 638, 'cream': 639, 'together': 640, '10': 641, 'hands': 642, 'theyll': 643, 'peanuts': 644, 'pay': 645, 'cheaper': 646, 'million': 647, 'smoking': 648, 'fires': 649, 'mayor': 650, 'election': 651, 'rained': 652, 'stars': 653, 'warm': 654, 'hundred': 655, 'stomach': 656, 'days': 657, 'learn': 658, 'talented': 659, 'liked': 660, 'laughing': 661, 'exciting': 662, 'score': 663, 'gone': 664, '6': 665, 'wow': 666, 'anyway': 667, 'throwing': 668, 'business': 669, 'empty': 670, 'month': 671, 'interesting': 672, 'houses': 673, 'bit': 674, 'slow': 675, 'shop': 676, 'penny': 677, 'miles': 678, 'sitting': 679, 'bed': 680, 'record': 681, 'shots': 682, 'buried': 683, 'apartment': 684, 'fridge': 685, 'sandwich': 686, 'paying': 687, 'book': 688, 'landed': 689, '200': 690, 'beer': 691, 'lucky': 692, 'patch': 693, 'ones': 694, 'whys': 695, 'driving': 696, 'takes': 697, 'mud': 698, 'teacher': 699, 'killed': 700, 'hood': 701, 'red': 702, 'sign': 703, 'hurry': 704, 'park': 705, 'players': 706, 'online': 707, 'early': 708, '1': 709, 'nails': 710, 'weve': 711, 'golf': 712, 'pair': 713, 'move': 714, 'smoke': 715, 'pets': 716, 'airport': 717, 'ice': 718, 'ii': 719, 'hamburgers': 720, 'pepper': 721, 'cows': 722, 'ate': 723, 'banana': 724, 'voting': 725, 'stress': 726, 'ugly': 727, 'summer': 728, 'smells': 729, 'clearly': 730, 'plan': 731, 'sometime': 732, 'hang': 733, 'met': 734, 'medicine': 735, 'cute': 736, 'basketball': 737, 'missed': 738, 'able': 739, 'assignments': 740, 'switch': 741, 'alone': 742, 'visited': 743, 'saturday': 744, 'wheres': 745, 'wife': 746, 'expensive': 747, 'funnylooking': 748, 'letter': 749, 'church': 750, 'hotel': 751, 'drink': 752, 'cuts': 753, 'young': 754, 'channels': 755, 'arent': 756, 'easier': 757, 'whenever': 758, 'space': 759, 'carpet': 760, '100': 761, 'ten': 762, 'gravity': 763, 'important': 764, 'writing': 765, 'huge': 766, 'thieves': 767, 'math': 768, 'wipes': 769, 'crosswalk': 770, 'six': 771, 'flying': 772, 'order': 773, 'wear': 774, 'case': 775, 'running': 776, 'town': 777, 'ears': 778, 'french': 779, 'tomato': 780, 'stains': 781, 'tiger': 782, 'hospitals': 783, 'head': 784, 'battery': 785, 'jail': 786, 'fall': 787, 'article': 788, 'pink': 789, 'solve': 790, 'thinks': 791, 'doctors': 792, 'government': 793, 'pcc': 794, 'luck': 795, 'recently': 796, 'weird': 797, 'ninety': 798, 'seem': 799, 'unpredictable': 800, 'apologize': 801, 'brown': 802, 'short': 803, 'girls': 804, 'quite': 805, 'stayed': 806, 'excited': 807, 'congratulations': 808, 'appreciate': 809, 'macys': 810, 'dollars': 811, 'knew': 812, 'brought': 813, 'rock': 814, 'rb': 815, 'instruments': 816, '10198': 817, 'pass': 818, 'eye': 819, 'return': 820, 'ounces': 821, 'stopping': 822, 'bother': 823, 'invite': 824, 'sound': 825, 'pm': 826, 'parties': 827, 'rush': 828, 'enjoyed': 829, 'set': 830, 'dive': 831, 'turned': 832, 'joking': 833, 'small': 834, 'wonderful': 835, 'comfortable': 836, 'drinking': 837, 'computer': 838, 'ocean': 839, '1000': 840, 'spend': 841, 'mouth': 842, 'cares': 843, 'meant': 844, 'asleep': 845, 'others': 846, 'stuff': 847, 'needs': 848, 'cell': 849, 'stole': 850, 'mcdonalds': 851, 'extra': 852, 'towels': 853, 'dry': 854, 'market': 855, 'apples': 856, 'ham': 857, 'crashed': 858, '85': 859, 'plus': 860, 'bluedog123': 861, 'dream': 862, 'son': 863, 'everywhere': 864, 'yell': 865, 'simple': 866, 'wet': 867, 'machine': 868, 'usually': 869, 'nation': 870, 'flew': 871, 'race': 872, 'children': 873, 'pencils': 874, 'famous': 875, 'third': 876, 'women': 877, 'green': 878, 'poems': 879, 'songs': 880, 'court': 881, 'cemetery': 882, 'dead': 883, 'vacuum': 884, 'service': 885, 'dial': 886, 'desk': 887, 'glass': 888, 'kidding': 889, 'teaching': 890, 'drove': 891, 'spot': 892, 'books': 893, 'add': 894, 'taking': 895, 'stupid': 896, 'ran': 897, '3': 898, 'hospital': 899, 'riding': 900, 'bus': 901, 'student': 902, 'speed': 903, 'accident': 904, 'hamburger': 905, 'traffic': 906, '400': 907, 'drivers': 908, 'anywhere': 909, 'stopped': 910, 'cart': 911, 'uses': 912, 'wins': 913, 'neighbors': 914, 'goodness': 915, 'streets': 916, 'yard': 917, 'tells': 918, 'guy': 919, 'shows': 920, 'sit': 921, 'follows': 922, 'works': 923, 'interested': 924, 'spit': 925, 'door': 926, 'napkin': 927, 'worth': 928, 'lake': 929, 'relax': 930, 'person': 931, 'lesson': 932, 'law': 933, 'breathe': 934, 'rest': 935, 'receipt': 936, 'doors': 937, 'earthquakes': 938, 'florida': 939, 'october': 940, 'breakfast': 941, 'eggs': 942, 'during': 943, 'selling': 944, 'hurts': 945, 'stuck': 946, '90': 947, 'soldiers': 948, 'place': 949, 'become': 950, 'sell': 951, 'company': 952, 'd': 953, 'corporations': 954, 'heres': 955, 'weight': 956, 'solution': 957, 'sent': 958, 'corner': 959, 'candidate': 960, 'ads': 961, 'blood': 962, 'pale': 963, 'smoker': 964, 'brush': 965, 'picking': 966, 'attend': 967, 'sooner': 968, 'impossible': 969, 'speak': 970, 'calling': 971, 'seeing': 972, 'upset': 973, 'forty': 974, 'chucks': 975, 'learned': 976, 'talent': 977, 'sort': 978, 'mad': 979, 'none': 980, 'ideas': 981, 'decided': 982, 'answer': 983, 'plans': 984, 'done': 985, 'broke': 986, 'married': 987, 'living': 988, 'instead': 989, '30000': 990, 'write': 991, 'stamps': 992, 'feed': 993, 'cats': 994, 'miss': 995, 'thirteenth': 996, 'floor': 997, 'born': 998, 'tax': 999, 'email': 1000, 'yikes': 1001, 'hmm': 1002, 'meet': 1003, 'seven': 1004, 'except': 1005, 'glue': 1006, 'ear': 1007, 'death': 1008, 'ink': 1009, 'faster': 1010, '140': 1011, 'fault': 1012, 'worse': 1013, 'wind': 1014, 'cheap': 1015, 'officer': 1016, 'player': 1017, 'chilly': 1018, 'chase': 1019, 'gas': 1020, 'along': 1021, 'wants': 1022, 'bill': 1023, 'caught': 1024, 'cook': 1025, 'workers': 1026, 'manager': 1027, 'immediately': 1028, 'salad': 1029, 'foul': 1030, 'holes': 1031, 'practice': 1032, 'tournament': 1033, 'neighborhood': 1034, 'upstairs': 1035, 'season': 1036, 'bomb': 1037, 'flip': 1038, 'poor': 1039, 'dressing': 1040, 'pasta': 1041, 'vegetables': 1042, 'strong': 1043, 'bananas': 1044, '99': 1045, 'peanut': 1046, 'hed': 1047, 'bears': 1048, 'obama': 1049, 'voters': 1050, 'lie': 1051, 'fingers': 1052, 'finger': 1053, 'drops': 1054, 'rub': 1055, 'degrees': 1056, 'horrible': 1057, 'clear': 1058, 'cleaner': 1059, 'changing': 1060, 'somewhere': 1061, 'special': 1062, 'noticed': 1063, 'seriously': 1064, 'deserved': 1065, 'truth': 1066, 'offered': 1067, 'drawing': 1068, 'painting': 1069, 'often': 1070, 'funniest': 1071, 'laugh': 1072, 'honestly': 1073, 'hilarious': 1074, 'types': 1075, 'played': 1076, 'nights': 1077, 'planning': 1078, 'trouble': 1079, 'favor': 1080, 'mention': 1081, 'wouldve': 1082, 'somebody': 1083, 'debrah': 1084, 'subject': 1085, 'random': 1086, 'shell': 1087, 'invitations': 1088, 'loads': 1089, '800': 1090, 'hoping': 1091, 'los': 1092, 'angeles': 1093, 'poodle': 1094, 'poodles': 1095, 'bark': 1096, 'shut': 1097, 'wallet': 1098, 'mountains': 1099, 'mattress': 1100, 'window': 1101, 'variety': 1102, 'deep': 1103, 'top': 1104, 'boyfriend': 1105, 'birthday': 1106, 'ha': 1107, 'sink': 1108, 'tub': 1109, 'counter': 1110, 'drawer': 1111, 'yawning': 1112, 'sleepy': 1113, 'commercials': 1114, 'forgot': 1115, 'blades': 1116, 'electric': 1117, 'beard': 1118, 'sticks': 1119, 'reading': 1120, 'eats': 1121, 'trust': 1122, 'happens': 1123, 'months': 1124, 'prove': 1125, 'drop': 1126, 'begin': 1127, 'ripe': 1128, 'thrift': 1129, 'pant': 1130, 'cuffs': 1131, 'washed': 1132, 'dryer': 1133, 'mostly': 1134, 'oranges': 1135, 'cabinet': 1136, 'screen': 1137, 'files': 1138, '60': 1139, 'address': 1140, 'state': 1141, 'nap': 1142, 'sleep': 1143, 'cooking': 1144, 'tired': 1145, 'speech': 1146, 'lonely': 1147, 'chop': 1148, 'jerks': 1149, 'mi': 1150, 'example': 1151, 'birth': 1152, '12': 1153, 'showed': 1154, 'gray': 1155, 'noon': 1156, 'heat': 1157, 'snow': 1158, 'recycle': 1159, 'converter': 1160, 'canada': 1161, 'fighter': 1162, 'crash': 1163, 'shoot': 1164, 'robber': 1165, 'hair': 1166, 'sleeves': 1167, 'artists': 1168, 'jar': 1169, 'artist': 1170, 'lessons': 1171, 'drew': 1172, 'offer': 1173, 'tough': 1174, 'taste': 1175, 'pens': 1176, 'onto': 1177, 'shoe': 1178, 'sharp': 1179, 'knife': 1180, 'criminals': 1181, 'sewing': 1182, 'joke': 1183, 'spanish': 1184, 'showers': 1185, 'test': 1186, 'ridiculous': 1187, 'license': 1188, 'chased': 1189, 'supermarket': 1190, 'cremated': 1191, 'flags': 1192, 'plants': 1193, 'distance': 1194, 'teachers': 1195, 'classmates': 1196, 'checked': 1197, 'pour': 1198, 'glasses': 1199, 'appointment': 1200, 'pages': 1201, 'dictionary': 1202, 'classroom': 1203, 'graduate': 1204, 'army': 1205, 'rooms': 1206, 'belongings': 1207, 'pray': 1208, 'pushed': 1209, 'gently': 1210, 'nearby': 1211, '911': 1212, 'windows': 1213, 'strange': 1214, 'buses': 1215, 'crowded': 1216, 'crossing': 1217, 'limit': 1218, '65': 1219, '75': 1220, 'cop': 1221, 'flat': 1222, 'luxury': 1223, 'keys': 1224, 'between': 1225, 'trucks': 1226, 'blows': 1227, 'kill': 1228, 'straight': 1229, 'low': 1230, 'owned': 1231, 'speeding': 1232, 'killing': 1233, 'injured': 1234, 'carts': 1235, 'metal': 1236, 'saved': 1237, 'cry': 1238, 'games': 1239, 'licks': 1240, 'lips': 1241, 'playing': 1242, 'dome': 1243, 'bitter': 1244, 'step': 1245, 'jacket': 1246, 'glove': 1247, 'warmer': 1248, '80': 1249, 'per': 1250, 'bear': 1251, 'amazing': 1252, 'common': 1253, 'homeless': 1254, 'color': 1255, 'actors': 1256, 'curse': 1257, 'broadcasting': 1258, 'dozen': 1259, 'difference': 1260, 'cd': 1261, 'cable': 1262, 'sight': 1263, 'standing': 1264, 'behind': 1265, 'pineapples': 1266, 'fact': 1267, 'worst': 1268, 'tip': 1269, 'dating': 1270, 'sneak': 1271, 'evening': 1272, 'chinese': 1273, 'popular': 1274, 'burger': 1275, 'soup': 1276, 'tasted': 1277, 'bacon': 1278, 'sandwiches': 1279, 'toast': 1280, 'main': 1281, 'send': 1282, 'certainly': 1283, 'ground': 1284, '18': 1285, 'bases': 1286, 'slide': 1287, 'yankees': 1288, 'boring': 1289, 'greatest': 1290, 'scuba': 1291, 'sank': 1292, 'stroke': 1293, 'human': 1294, 'shot': 1295, 'ruth': 1296, 'punched': 1297, 'tonight': 1298, '7': 1299, 'tickets': 1300, 'apologized': 1301, 'sense': 1302, 'bulb': 1303, 'slip': 1304, 'cleanup': 1305, 'quickly': 1306, 'roll': 1307, 'stove': 1308, 'rate': 1309, 'hurricanes': 1310, 'hawaii': 1311, 'sheet': 1312, 'freeways': 1313, 'york': 1314, 'christmas': 1315, 'sneezing': 1316, 'cabin': 1317, 'sister': 1318, 'trail': 1319, 'threat': 1320, 'invited': 1321, 'veterans': 1322, 'monument': 1323, 'pictures': 1324, '300': 1325, 'teeth': 1326, 'match': 1327, 'dark': 1328, 'making': 1329, '4000': 1330, 'stock': 1331, 'least': 1332, 'knock': 1333, 'diet': 1334, 'natural': 1335, 'prepare': 1336, '15': 1337, 'fruits': 1338, 'boiled': 1339, 'soft': 1340, 'belt': 1341, 'tag': 1342, 'fit': 1343, 'pound': 1344, 'swiss': 1345, 'handle': 1346, 'produce': 1347, 'usual': 1348, 'collision': 1349, '2': 1350, 'unit': 1351, 'allowed': 1352, 'woods': 1353, 'berries': 1354, 'promises': 1355, 'votes': 1356, '11': 1357, 'ralph': 1358, 'nader': 1359, 'hates': 1360, 'elected': 1361, 'possible': 1362, 'explain': 1363, 'honest': 1364, 'against': 1365, 'measures': 1366, 'title': 1367, 'pain': 1368, 'causes': 1369, 'positive': 1370, 'medication': 1371, 'sun': 1372, 'chain': 1373, 'flossing': 1374, 'pimples': 1375, 'swine': 1376, 'flu': 1377, 'scratching': 1378, 'cloth': 1379, 'hows': 1380, 'hopefully': 1381, 'closer': 1382, 'itll': 1383, 'hello': 1384, 'alice': 1385, 'stomachache': 1386, 'boss': 1387, 'wearing': 1388, 'santa': 1389, 'felt': 1390, 'certain': 1391, 'sucks': 1392, 'attended': 1393, 'sad': 1394, 'hey': 1395, 'jessicas': 1396, 'definitely': 1397, 'oil': 1398, 'borrow': 1399, '5': 1400, 'warned': 1401, 'shaving': 1402, 'noise': 1403, 'excuse': 1404, 'puppies': 1405, 'unlucky': 1406, 'reservation': 1407, 'angry': 1408, 'laundry': 1409, 'pillowcases': 1410, 'pillows': 1411, 'bath': 1412, 'lady': 1413, 'perfume': 1414, 'hp': 1415, 'funeral': 1416, 'cheat': 1417, 'toes': 1418, 'bank': 1419, 'pilot': 1420, 'cops': 1421, 'finding': 1422, 'sleeve': 1423, 'drug': 1424, 'prefer': 1425, 'sew': 1426, 'plastic': 1427, 'poetry': 1428, 'supersmart': 1429, 'rules': 1430, 'major': 1431, 'library': 1432, 'finally': 1433, 'accidents': 1434, 'honk': 1435, 'birds': 1436, 'afternoon': 1437, 'dent': 1438, 'cards': 1439, 'tens': 1440, 'cap': 1441, 'colder': 1442, 'hasnt': 1443, 'sofa': 1444, 'stood': 1445, 'group': 1446, 'pbs': 1447, 'public': 1448, 'donate': 1449, 'seller': 1450, 'female': 1451, 'antenna': 1452, 'rabbit': 1453, 'trade': 1454, 'stink': 1455, 'fill': 1456, 'silly': 1457, 'fishing': 1458, 'golfer': 1459, 'whos': 1460, 'babe': 1461, 'burned': 1462, 'alarm': 1463, 'seatbelt': 1464, 'complain': 1465, 'puddle': 1466, 'guns': 1467, 'destroyed': 1468, 'homes': 1469, 'sleeping': 1470, 'prices': 1471, 'cruise': 1472, 'kid': 1473, 'mule': 1474, 'stunk': 1475, 'private': 1476, 'reporters': 1477, 'jobs': 1478, 'finished': 1479, 'chemistry': 1480, 'germs': 1481, 'salads': 1482, 'brother': 1483, 'careful': 1484, 'figure': 1485, 'carton': 1486, 'yours': 1487, 'pencil': 1488, 'sharpener': 1489, 'bag': 1490, 'twice': 1491, 'wed': 1492, 'quiet': 1493, 'completely': 1494, 'cow': 1495, 'cans': 1496, 'ballot': 1497, 'changes': 1498, 'country': 1499, 'promise': 1500, 'bush': 1501, 'fireman': 1502, 'typing': 1503, 'cancer': 1504, 'packs': 1505, 'brushing': 1506, 'hike': 1507, 'pimple': 1508, 'suck': 1509, 'emergency': 1510, 'remote': 1511, 'hi': 1512, 'campus': 1513, 'attending': 1514, 'enjoying': 1515, 'lovely': 1516, 'everythings': 1517, 'classes': 1518, 'since': 1519, 'considering': 1520, 'degree': 1521, 'pointless': 1522, 'clears': 1523, 'perfectly': 1524, 'activities': 1525, 'planned': 1526, 'ahead': 1527, 'forecast': 1528, 'ruin': 1529, 'badly': 1530, 'constantly': 1531, 'stays': 1532, 'uncertain': 1533, 'whatll': 1534, 'cleaning': 1535, 'speaking': 1536, 'answered': 1537, 'chores': 1538, 'describe': 1539, 'height': 1540, 'mightve': 1541, 'bumped': 1542, 'prettiest': 1543, 'facial': 1544, 'features': 1545, 'bothering': 1546, 'pepto': 1547, 'bismol': 1548, 'star': 1549, 'chuck': 1550, 'taylors': 1551, 'anita': 1552, 'mall': 1553, 'brand': 1554, 'picked': 1555, 'known': 1556, 'taught': 1557, 'theaters': 1558, 'tears': 1559, 'super': 1560, 'lying': 1561, 'throughout': 1562, 'hysterically': 1563, 'muscles': 1564, 'afterwards': 1565, 'instance': 1566, 'excellent': 1567, 'various': 1568, 'genres': 1569, 'interests': 1570, 'unable': 1571, 'intense': 1572, 'ended': 1573, 'winning': 1574, 'victorious': 1575, 'final': 1576, 'points': 1577, 'mustve': 1578, 'inviting': 1579, 'theater': 1580, 'catches': 1581, 'whether': 1582, 'welcome': 1583, 'cousin': 1584, 'labor': 1585, 'odd': 1586, 'regardless': 1587, 'basically': 1588, 'invites': 1589, 'guessing': 1590, 'intend': 1591, 'awesome': 1592, 'realize': 1593, 'harsh': 1594, 'besides': 1595, 'northern': 1596, 'southern': 1597, '140000': 1598, 'honda': 1599, '2003': 1600, 'moms': 1601, 'lend': 1602, 'drowned': 1603, 'lifeguard': 1604, 'swam': 1605, 'watched': 1606, 'rose': 1607, 'parade': 1608, 'restaurants': 1609, 'toss': 1610, 'marks': 1611, 'arms': 1612, 'bites': 1613, 'bite': 1614, 'bedbugs': 1615, 'throw': 1616, 'thingsnot': 1617, 'pepperoni': 1618, 'payment': 1619, 'thirty': 1620, 'thousand': 1621, 'herself': 1622, 'ring': 1623, 'las': 1624, 'vegas': 1625, 'gambling': 1626, 'toilet': 1627, 'wasting': 1628, 'volume': 1629, 'grandma': 1630, 'sealed': 1631, 'tape': 1632, 'recorder': 1633, 'broken': 1634, 'rerun': 1635, 'original': 1636, 'ends': 1637, 'coat': 1638, 'tie': 1639, 'respect': 1640, 'meowing': 1641, 'themselves': 1642, 'rid': 1643, 'blade': 1644, 'shaver': 1645, 'shave': 1646, 'plate': 1647, 'unhappy': 1648, 'mistake': 1649, 'hassle': 1650, 'mother': 1651, 'moved': 1652, 'peoples': 1653, '42': 1654, '44': 1655, 'shirts': 1656, 'chocolate': 1657, 'mirror': 1658, 'dried': 1659, 'folded': 1660, 'beds': 1661, 'current': 1662, 'events': 1663, 'tasty': 1664, 'mustard': 1665, 'potato': 1666, 'chips': 1667, 'smelled': 1668, 'installation': 1669, 'remove': 1670, 'replace': 1671, 'screws': 1672, 'incomplete': 1673, 'mailing': 1674, '456': 1675, 'cherry': 1676, 'ca': 1677, '91170': 1678, 'correct': 1679, 'zip': 1680, 'code': 1681, 'unplug': 1682, '600': 1683, 'awake': 1684, '45': 1685, 'word': 1686, 'elephant': 1687, 'deaf': 1688, 'twentyfive': 1689, 'share': 1690, 'cheater': 1691, 'poke': 1692, 'yelling': 1693, 'stands': 1694, 'initial': 1695, 'mmddyy': 1696, 'monthdayyear': 1697, 'january': 1698, '1987': 1699, '011287': 1700, 'drag': 1701, 'woke': 1702, 'hotter': 1703, 'dying': 1704, 'conditioner': 1705, 'repairman': 1706, 'snowman': 1707, 'carrot': 1708, 'withdraw': 1709, 'atm': 1710, 'automatic': 1711, 'teller': 1712, 'insert': 1713, 'debit': 1714, 'truck': 1715, 'tuesday': 1716, 'switching': 1717, '120': 1718, '13inch': 1719, 'jets': 1720, 'followed': 1721, 'highway': 1722, 'walked': 1723, 'sucked': 1724, 'jet': 1725, 'reporting': 1726, 'robbery': 1727, 'racist': 1728, 'identify': 1729, 'tissues': 1730, 'daddy': 1731, 'marriage': 1732, 'responsibility': 1733, 'differently': 1734, 'picasso': 1735, 'painted': 1736, 'paintings': 1737, 'grade': 1738, 'nothings': 1739, 'carries': 1740, 'ironon': 1741, 'iron': 1742, 'melts': 1743, 'language': 1744, 'languages': 1745, 'speaks': 1746, 'learning': 1747, 'april': 1748, '22': 1749, 'anniversary': 1750, 'earth': 1751, 'yearly': 1752, 'reminder': 1753, 'planet': 1754, 'reuse': 1755, 'bags': 1756, 'shorter': 1757, 'waste': 1758, 'rhyme': 1759, 'buckle': 1760, 'shakespeare': 1761, 'poet': 1762, 'average': 1763, 'iq': 1764, 'bs': 1765, 'lead': 1766, 'actress': 1767, 'second': 1768, 'actor': 1769, 'daughter': 1770, 'bull': 1771, 'octomom': 1772, 'avoid': 1773, 'ashes': 1774, 'shaken': 1775, 'coffin': 1776, 'seldom': 1777, 'wiped': 1778, 'shoestheyre': 1779, 'dries': 1780, 'mothers': 1781, 'raised': 1782, 'stripes': 1783, 'watered': 1784, 'vacuumed': 1785, 'entire': 1786, 'calls': 1787, 'force': 1788, 'pulls': 1789, 'float': 1790, 'balloon': 1791, 'prescription': 1792, 'yellow': 1793, 'notebook': 1794, 'calculator': 1795, 'permit': 1796, 'calculators': 1797, 'subscribed': 1798, 'political': 1799, 'cartoons': 1800, 'photos': 1801, 'film': 1802, 'reviews': 1803, 'section': 1804, 'subscription': 1805, 'canceled': 1806, 'magazines': 1807, 'shake': 1808, 'shook': 1809, 'joining': 1810, 'totally': 1811, 'yelled': 1812, 'backpack': 1813, 'ipod': 1814, 'occasionally': 1815, 'whens': 1816, 'prayers': 1817, 'passed': 1818, 'tests': 1819, 'instantly': 1820, 'placed': 1821, 'department': 1822, 'seat': 1823, '24': 1824, 'aisle': 1825, 'unsafe': 1826, 'eleven': 1827, 'fair': 1828, 'cross': 1829, 'stops': 1830, 'fastest': 1831, 'cadillac': 1832, 'longer': 1833, 'explode': 1834, 'bumps': 1835, 'parked': 1836, 'steal': 1837, 'complaining': 1838, 'horn': 1839, 'bucket': 1840, 'rinse': 1841, 'scrub': 1842, 'sponge': 1843, 'soap': 1844, 'towel': 1845, 'trees': 1846, 'dads': 1847, 'damage': 1848, 'pull': 1849, 'stone': 1850, 'leftturn': 1851, 'arrow': 1852, 'lane': 1853, 'uturn': 1854, 'reliable': 1855, 'mileage': 1856, 'registration': 1857, 'pulled': 1858, 'sudden': 1859, 'siren': 1860, 'rolled': 1861, 'slowed': 1862, 'attitude': 1863, 'downtown': 1864, 'jaywalking': 1865, 'entered': 1866, 'usc': 1867, 'passenger': 1868, 'continued': 1869, 'twelve': 1870, 'poker': 1871, 'nope': 1872, 'drives': 1873, 'quietly': 1874, 'remind': 1875, 'radios': 1876, 'chances': 1877, 'rainstorm': 1878, 'figured': 1879, 'sugar': 1880, 'gloves': 1881, 'slowing': 1882, 'pipe': 1883, 'mouse': 1884, 'weatherman': 1885, 'temperature': 1886, 'sports': 1887, 'fighting': 1888, 'computers': 1889, 'internet': 1890, 'china': 1891, 'wall': 1892, 'musical': 1893, 'sing': 1894, 'song': 1895, 'crowds': 1896, 'pursuit': 1897, 'happyness': 1898, 'based': 1899, 'starbucks': 1900, 'easter': 1901, '15000': 1902, 'wheelchairs': 1903, 'violence': 1904, 'action': 1905, 'system': 1906, 'puts': 1907, 'gardening': 1908, 'knitting': 1909, 'adding': 1910, 'judge': 1911, 'judy': 1912, 'lawsuits': 1913, 'complained': 1914, 'ebay': 1915, 'buyer': 1916, 'occur': 1917, 'signal': 1918, 'single': 1919, 'channel': 1920, 'personality': 1921, 'uhoh': 1922, 'weighs': 1923, '98': 1924, 'mood': 1925, 'boobs': 1926, 'checkout': 1927, 'gotten': 1928, 'shelf': 1929, 'jazz': 1930, 'club': 1931, 'pleasant': 1932, 'taxi': 1933, '40': 1934, 'meal': 1935, 'blame': 1936, 'act': 1937, 'hobby': 1938, 'furthest': 1939, 'laughed': 1940, 'goodlooking': 1941, 'goodsmelling': 1942, 'sensitive': 1943, 'odors': 1944, 'thatcigarettes': 1945, 'reservations': 1946, 'fastfood': 1947, 'slowest': 1948, 'lemon': 1949, 'pickles': 1950, 'rice': 1951, 'smile': 1952, 'brings': 1953, 'waiters': 1954, 'fingernails': 1955, 'disgusting': 1956, 'poured': 1957, 'yuck': 1958, 'cooks': 1959, 'charge': 1960, 'piece': 1961, 'serve': 1962, 'tables': 1963, 'chairs': 1964, 'silverware': 1965, 'passes': 1966, 'inspection': 1967, 'forgetting': 1968, 'examine': 1969, 'oops': 1970, 'drinks': 1971, 'adventure': 1972, 'bar': 1973, 'andy': 1974, 'warhol': 1975, 'butterflies': 1976, 'flower': 1977, 'golfers': 1978, 'mental': 1979, 'river': 1980, 'rods': 1981, 'bait': 1982, 'dodgers': 1983, 'drown': 1984, 'retires': 1985, 'dives': 1986, 'nervous': 1987, '20foot': 1988, 'putt': 1989, '25footer': 1990, 'outer': 1991, 'possibly': 1992, 'changed': 1993, 'cheer': 1994, 'argument': 1995, 'victim': 1996, 'concrete': 1997, 'steps': 1998, 'outfield': 1999, 'hitting': 2000, 'amateur': 2001, 'arm': 2002, 'follow': 2003, 'hitter': 2004, 'personal': 2005, 'fans': 2006, 'league': 2007, 'suspended': 2008, 'sidewalks': 2009, 'firebug': 2010, 'latest': 2011, 'charged': 2012, 'murder': 2013, 'protect': 2014, 'uncomfortable': 2015, 'tight': 2016, 'breath': 2017, 'burnt': 2018, 'dictionaries': 2019, 'textbooks': 2020, 'holding': 2021, 'breaks': 2022, 'pieces': 2023, 'slick': 2024, 'floors': 2025, 'slips': 2026, 'crack': 2027, 'cone': 2028, 'cones': 2029, 'north': 2030, 'rosters': 2031, 'burner': 2032, 'questions': 2033, 'lock': 2034, 'believes': 2035, 'recent': 2036, 'safer': 2037, 'june': 2038, 'harmless': 2039, 'island': 2040, 'swimming': 2041, 'sunny': 2042, 'sausage': 2043, 'juice': 2044, 'allow': 2045, 'leaves': 2046, '1215': 2047, '1015': 2048, '915': 2049, '815': 2050, 'fly': 2051, 'holidays': 2052, 'march': 2053, 'flown': 2054, 'jammed': 2055, 'coughing': 2056, 'elbow': 2057, 'knee': 2058, 'crying': 2059, 'climbing': 2060, 'zoo': 2061, 'someones': 2062, 'lasts': 2063, 'paid': 2064, 'depends': 2065, 'storms': 2066, 'view': 2067, 'security': 2068, 'land': 2069, 'altitude': 2070, 'earplugs': 2071, 'arizona': 2072, 'grand': 2073, 'canyon': 2074, 'walls': 2075, 'thin': 2076, 'tvs': 2077, 'telephones': 2078, 'snoring': 2079, 'housekeeping': 2080, 'nonsmoking': 2081, 'elevator': 2082, 'added': 2083, 'phony': 2084, 'charges': 2085, 'falling': 2086, 'washington': 2087, 'dc': 2088, 'organization': 2089, 'served': 2090, 'flight': 2091, 'band': 2092, 'honor': 2093, 'coworkers': 2094, 'newspaper': 2095, 'doublecheck': 2096, 'shine': 2097, 'shined': 2098, 'socks': 2099, 'babysitter': 2100, 'babies': 2101, 'diapers': 2102, 'painter': 2103, 'handyman': 2104, 'dripping': 2105, 'faucet': 2106, 'parts': 2107, 'skills': 2108, 'layoffs': 2109, 'flipping': 2110, 'king': 2111, 'part': 2112, 'routine': 2113, 'yells': 2114, 'jerk': 2115, 'report': 2116, 'supervisor': 2117, 'troublemakers': 2118, 'complainers': 2119, 'salary': 2120, 'lighters': 2121, 'fortune': 2122, 'guarantees': 2123, 'buys': 2124, 'hunch': 2125, 'savings': 2126, 'basket': 2127, 'f': 2128, 'tutor': 2129, 'killer': 2130, 'exercise': 2131, 'pet': 2132, 'unfriendly': 2133, 'areat': 2134, 'knocking': 2135, 'lettuce': 2136, 'celery': 2137, 'leather': 2138, 'meat': 2139, 'customer': 2140, 'ordered': 2141, 'sliced': 2142, 'switched': 2143, 'processed': 2144, 'vitamins': 2145, 'steam': 2146, 'girlfriends': 2147, 'sprinkled': 2148, 'feelings': 2149, 'peeled': 2150, 'sticker': 2151, 'navel': 2152, 'roasted': 2153, 'salted': 2154, 'boil': 2155, 'raw': 2156, 'shells': 2157, 'allergic': 2158, 'gained': 2159, 'filled': 2160, 'freezer': 2161, 'leftovers': 2162, 'disappear': 2163, 'reheated': 2164, 'burst': 2165, 'loosen': 2166, 'loosened': 2167, 'unbuttoned': 2168, 'clerk': 2169, 'kept': 2170, 'waist': 2171, 'bigger': 2172, 'elastic': 2173, 'waistband': 2174, 'list': 2175, 'nonfat': 2176, 'purse': 2177, 'purses': 2178, 'stronger': 2179, 'markets': 2180, 'offering': 2181, 'shoppers': 2182, 'deals': 2183, 'large': 2184, 'onepound': 2185, 'charity': 2186, 'desktop': 2187, 'pc': 2188, 'mac': 2189, 'macs': 2190, 'pcs': 2191, 'site': 2192, 'credit': 2193, 'sharpen': 2194, 'dining': 2195, 'sharpeners': 2196, 'legs': 2197, 'rubber': 2198, 'suction': 2199, 'cups': 2200, 'saves': 2201, 'mixed': 2202, 'cash': 2203, 'monthly': 2204, 'payments': 2205, 'inside': 2206, 'intersection': 2207, 'sides': 2208, 'barking': 2209, 'units': 2210, '9': 2211, 'doorbell': 2212, 'visitors': 2213, 'bedrooms': 2214, 'bathrooms': 2215, 'afraid': 2216, 'road': 2217, 'heated': 2218, 'survive': 2219, 'fireplaces': 2220, 'wood': 2221, 'weekends': 2222, 'lawns': 2223, 'rusty': 2224, 'yards': 2225, '3story': 2226, 'property': 2227, 'values': 2228, 'council': 2229, 'loses': 2230, 'starving': 2231, 'belong': 2232, 'grass': 2233, 'round': 2234, 'cover': 2235, 'trash': 2236, 'speaker': 2237, 'promised': 2238, 'officers': 2239, 'liar': 2240, '2000': 2241, 'closed': 2242, 'bowling': 2243, 'alley': 2244, 'helps': 2245, 'relaxed': 2246, 'leaders': 2247, 'mccain': 2248, 'obamas': 2249, 'imagine': 2250, 'spends': 2251, 'duties': 2252, 'traveling': 2253, 'visiting': 2254, 'cities': 2255, 'raising': 2256, 'reelection': 2257, 'democrats': 2258, 'republicans': 2259, 'chance': 2260, 'unless': 2261, 'officials': 2262, 'democratic': 2263, 'american': 2264, 'overseas': 2265, '40000': 2266, 'wounded': 2267, 'spoke': 2268, 'soldier': 2269, 'plenty': 2270, 'voter': 2271, 'instructions': 2272, '42cent': 2273, 'highways': 2274, 'prisons': 2275, 'legislators': 2276, 'spent': 2277, 'firstclass': 2278, 'improve': 2279, 'roads': 2280, 'raise': 2281, 'increase': 2282, 'opposite': 2283, 'measure': 2284, 'cereal': 2285, 'bleeding': 2286, 'ruined': 2287, 'causing': 2288, 'stomachaches': 2289, 'bandaids': 2290, 'bandaid': 2291, 'heals': 2292, 'stinks': 2293, 'sidewalk': 2294, 'smokers': 2295, 'weak': 2296, 'controls': 2297, 'puff': 2298, 'taken': 2299, 'tan': 2300, 'skin': 2301, 'surgery': 2302, 'insurance': 2303, 'unbelievable': 2304, 'alive': 2305, 'heart': 2306, 'lungs': 2307, 'healthy': 2308, 'chore': 2309, 'brushed': 2310, 'floss': 2311, 'invented': 2312, 'dentist': 2313, 'climb': 2314, 'wild': 2315, 'lizards': 2316, 'hiking': 2317, 'goats': 2318, 'sweat': 2319, 'genes': 2320, 'pollution': 2321, 'cause': 2322, 'newspapers': 2323, 'stories': 2324, 'fourth': 2325, 'mexico': 2326, 'covered': 2327, 'crud': 2328, 'damp': 2329, 'cracks': 2330, 'squeeze': 2331, 'firmly': 2332, 'swelling': 2333, 'normal': 2334, 'aches': 2335, 'righthanded': 2336, 'goodbye': 2337, 'america': 2338, 'nicer': 2339, 'available': 2340, 'werent': 2341, 'spare': 2342, 'hobbies': 2343, 'meaning': 2344, 'happening': 2345, 'conversing': 2346, 'bored': 2347, 'kittens': 2348, 'grandmas': 2349, 'somethings': 2350, 'form': 2351, 'animal': 2352, 'shelter': 2353, 'bin': 2354, 'powerful': 2355, 'magazine': 2356, 'ride': 2357, 'windy': 2358, 'near': 2359, 'titanic': 2360, 'lotto': 2361, 'beatles': 2362, 'singer': 2363, 'stations': 2364, 'hug': 2365, 'noses': 2366, 'jogging': 2367, 'fifteen': 2368, 'rings': 2369, 'buying': 2370, 'vacation': 2371, 'rowed': 2372, 'across': 2373, 'atlantic': 2374, 'ship': 2375, 'spring': 2376, 'interview': 2377, 'carrier': 2378, 'deli': 2379, 'gaining': 2380, 'stuffed': 2381, 'ripped': 2382, 'stretch': 2383, 'barbara': 2384, 'invading': 2385, 'neighborhoods': 2386, 'senator': 2387, 'former': 2388, 'conference': 2389, 'official': 2390, 'sample': 2391, 'stain': 2392, 'suntan': 2393, 'lotion': 2394, 'smokes': 2395, 'filthy': 2396, 'predictable': 2397, 'hidden': 2398, 'shouldve': 2399, 'letting': 2400, 'doubt': 2401, 'bye': 2402, 'mechanic': 2403, 'watchdogs': 2404, 'pennies': 2405, 'clothes': 2406, 'zzz': 2407, 'forgive': 2408, 'lick': 2409, 'blacky': 2410, 'dies': 2411, 'candy': 2412, 'bars': 2413, 'pickle': 2414, 'crashes': 2415, 'bluedog123yahoocom': 2416, 'honey': 2417, 'swear': 2418, 'print': 2419, 'bubbles': 2420, 'name': 2421, 'umbrella': 2422, 'korean': 2423, 'male': 2424, 'sexist': 2425, 'mommy': 2426, 'millions': 2427, 'patient': 2428, 'washings': 2429, 'washes': 2430, 'ahora': 2431, 'shower': 2432, 'understands': 2433, 'saying': 2434, 'nanny': 2435, 'infants': 2436, 'carrying': 2437, 'hearing': 2438, 'turning': 2439, 'himself': 2440, 'rob': 2441, 'drunks': 2442, 'caused': 2443, 'camera': 2444, 'exit': 2445, 'towed': 2446, 'quicker': 2447, 'saving': 2448, 'fight': 2449, 'witness': 2450, '150': 2451, 'cheating': 2452, 'ending': 2453, 'sniff': 2454, 'cds': 2455, 'problemsshes': 2456, 'snails': 2457, 'cateyes': 2458, 'apply': 2459, 'customs': 2460, 'focus': 2461, 'sat': 2462, 'nuts': 2463, 'mars': 2464, 'figures': 2465, 'moves': 2466, 'stepladder': 2467, 'mop': 2468, 'clock': 2469, 'hurricane': 2470, 'andrew': 2471, 'fallen': 2472, 'agent': 2473, '50percent': 2474, 'discount': 2475, 'someday': 2476, 'donations': 2477, 'deserve': 2478, 'repeat': 2479, 'choose': 2480, 'happiness': 2481, 'somethingyoull': 2482, 'doctorsno': 2483, 'patients': 2484, 'peoplenot': 2485, 'sore': 2486, 'knuckles': 2487, 'calories': 2488, 'mans': 2489, 'huh': 2490, 'sauce': 2491, 'strict': 2492, 'losing': 2493, 'google': 2494, 'search': 2495, 'bills': 2496, 'anytime': 2497, 'tastes': 2498, 'less': 2499, 'checkbook': 2500, 'rent': 2501, 'shooting': 2502, 'leads': 2503, 'reelected': 2504, 'office': 2505, 'thattheir': 2506, 'giving': 2507, 'liethats': 2508, 'politicians': 2509, 'soak': 2510, 'bone': 2511, 'sand': 2512, 'heal': 2513, 'arguing': 2514, 'bright': 2515, 'side': 2516, 'pop': 2517, 'frequently': 2518, 'planes': 2519, 'bless': 2520, 'modern': 2521, 'starttoken': 1, 'endtoken': 2}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "Input: upset?\n",
            "Response:  solve solve solve solve solve solve  write                     \n"
          ]
        }
      ]
    }
  ]
}
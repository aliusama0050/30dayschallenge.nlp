{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TASK2"
      ],
      "metadata": {
        "id": "AsPkU9ISUbF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZATION**"
      ],
      "metadata": {
        "id": "44kLji3RY29A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WILL HAVE TO PERFORM TOKENIZATION WICH MEANS BRAEKING THE TEXT INTO SMALL UNITS CAN BREAK IN WORDS OR EVEN CHARACTERS BASED ON REQUIREMENTS . TOKENIZATION PROCESS IS IMPORTANT BECAUSE WE HAVE TO CONVERT UNSTRUCTERED FORM OF DATA INTO MEANINGFUL WHICH COMPUTER EASILY CAN UNDERSTAND WE WILL USE TWO LIBRARIES FOR THIS PURPOSE"
      ],
      "metadata": {
        "id": "m7visi5AUqDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.**NLTK**.\n",
        "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP\n",
        "It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning\n",
        "\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "PrGSeDlAVhs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gefPr8Qfaeek",
        "outputId": "6f10edfa-7eb9-4c68-c2ed-28b23658d263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "text = \"MY NAME IS ALI USAMA MY FAVOURITE SPORT IS CRICKET AND PASSION IS PROGRAMMING\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZtREFemY0Ws",
        "outputId": "e1a3cc3f-cf2f-45dc-b5e4-f4bd76fcb03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MY', 'NAME', 'IS', 'ALI', 'USAMA', 'MY', 'FAVOURITE', 'SPORT', 'IS', 'CRICKET', 'AND', 'PASSION', 'IS', 'PROGRAMMING']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SPACY**\n",
        "spaCy is structured like a service. This means that it provides a precise solution for every problem. In practice, this means that developers can complete specific tasks quickly and easily with spaCy.It also contains pre-trained models for various languages. In total, spaCy supports more than 60 languages, including German, English, Spanish, Portuguese, Italian, French, Dutch and Greek."
      ],
      "metadata": {
        "id": "0PzL5JaFar5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"MY NAME IS ALI USAMA MY FAVOURITE SPORT IS CRICKET AND PASSION IS PROGRAMMING\"\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcaiit8nc9jJ",
        "outputId": "e66cb8cd-f3ce-462b-f95c-0955e314c82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MY', 'NAME', 'IS', 'ALI', 'USAMA', 'MY', 'FAVOURITE', 'SPORT', 'IS', 'CRICKET', 'AND', 'PASSION', 'IS', 'PROGRAMMING']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3:**\n",
        "\n",
        "Stop words removal is an essential step in text preprocessing for several reasons:\n",
        "\n",
        "1. Reducing Noise: Stop words are commonly occurring words that do not provide much information about the content of the text. Removing them helps reduce noise and focus on the more meaningful words in the document.\n",
        "\n",
        "2. Improving Performance: By removing stop words, the size of the vocabulary is reduced, which can lead to faster and more efficient processing of text data, especially in tasks like text classification, information retrieval, or sentiment analysis.\n",
        "\n",
        "3. Improving Accuracy: Stop words often occur in similar frequencies across different documents and may not contribute much to distinguishing between them. Removing stop words can improve the accuracy of NLP tasks by focusing on more informative words.\n",
        "\n",
        "Stop words can be handled during text preprocessing using various techniques:\n",
        "\n",
        "1. Manual Stop Words List: A predefined list of stop words specific to the language being analyzed can be created and used to filter out these words from the text. Libraries like NLTK provide built-in lists of stop words for different languages.\n",
        "\n",
        "2. Library-Based Stop Words Removal: NLP libraries like NLTK, spaCy, and scikit-learn offer functions or modules for removing stop words from text data. These libraries often provide pre-defined lists of stop words for different languages and easy-to-use functions for stop words removal.\n",
        "\n",
        "3. Custom Stop Words Removal: In some cases, it may be necessary to create a custom list of stop words tailored to the specific domain or context of the text data. This can be achieved by analyzing the frequency distribution of words in the corpus and identifying common stop words.\n",
        "\n",
        "Here's an example of how to remove stop words using NLTK in Python:\n"
      ],
      "metadata": {
        "id": "I5NvPdQIsVKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords for English\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"This is an example sentence demonstrating the removal of stop words.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords from the tokens\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqqxfLjkfXei",
        "outputId": "910552bc-d43f-4699-8ea1-1d343ffc6cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrating', 'removal', 'stop', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TASK4:**\n",
        "1. Stemming:\n",
        "\n",
        "   - Stemming is a rule-based process of reducing words to their base or root forms by removing suffixes. It aims to chop off the ends of words to produce the stem, which may not always be a valid word.\n",
        "   \n",
        "   - Stemming algorithms apply simple rules to strip off common suffixes, such as \"-ing,\" \"-ed,\" or \"-s,\" without considering the context of the word. This can sometimes result in stems that are not actual words.\n",
        "   \n",
        "   - Stemming is generally faster and less resource-intensive compared to lemmatization because it applies heuristic rules rather than accessing a language's full dictionary.\n",
        "   \n",
        "   - Example: For the word \"running,\" the stemmer might return \"run,\" and for \"better,\" it might return \"better.\"\n",
        "\n",
        "2. Lemmatization:\n",
        "\n",
        "   - Lemmatization, on the other hand, is a more sophisticated process that reduces words to their base or dictionary form, known as the lemma. It takes into account the morphological analysis of words and considers their context to determine the lemma.\n",
        "   \n",
        "   - Lemmatization typically involves dictionary lookup and linguistic analysis to correctly identify the lemma of a word, considering factors such as part of speech (POS) tags and word meanings.\n",
        "   \n",
        "   - Lemmatization ensures that the resulting lemma is a valid word in the language, providing more accurate and meaningful representations compared to stemming.\n",
        "   \n",
        "   - Example: For the word \"better,\" the lemmatizer would return \"good,\" and for \"running,\" it would return \"run.\"\n",
        "   \n",
        "In summary, the main differences between stemming and lemmatization are:\n",
        "\n",
        "- Approach: Stemming applies heuristic rules to chop off suffixes, while lemmatization involves dictionary lookup and linguistic analysis to determine the lemma.\n",
        "  \n",
        "- Outcome: Stemming may produce stems that are not actual words, while lemmatization always returns valid dictionary words.\n",
        "  \n",
        "- Accuracy: Lemmatization is generally more accurate and linguistically informed compared to stemming.\n",
        "  \n",
        "Despite these differences, both stemming and lemmatization serve the common goal of reducing words to their base forms, which is useful for tasks like text normalization, feature extraction, and improving the performance of NLP models.\n",
        "\n",
        "Here's an example of how to implement stemming and lemmatization using NLTK in Python:\n",
        "\n"
      ],
      "metadata": {
        "id": "Iys1A3gssrn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"Playing and played are both forms of play in English grammar. Plays, playing, and player are also related terms in this context.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform Stemming\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Perform Lemmatization\n",
        "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZIS8PH_tHoi",
        "outputId": "2157d6c2-0c9e-493a-da48-ccb53593c14c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words:\n",
            "['play', 'and', 'play', 'are', 'both', 'form', 'of', 'play', 'in', 'english', 'grammar', '.', 'play', ',', 'play', ',', 'and', 'player', 'are', 'also', 'relat', 'term', 'in', 'thi', 'context', '.']\n",
            "\n",
            "Lemmatized Words:\n",
            "['Playing', 'and', 'played', 'are', 'both', 'form', 'of', 'play', 'in', 'English', 'grammar', '.', 'Plays', ',', 'playing', ',', 'and', 'player', 'are', 'also', 'related', 'term', 'in', 'this', 'context', '.']\n"
          ]
        }
      ]
    }
  ]
}
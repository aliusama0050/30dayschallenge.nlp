{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzgIKOZfap18",
        "outputId": "59ddd48c-4422-449f-f7dc-c48e45e5e0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "19/19 [==============================] - 5s 120ms/step - loss: 5.7544 - accuracy: 0.0409\n",
            "Epoch 2/15\n",
            "19/19 [==============================] - 3s 159ms/step - loss: 5.4329 - accuracy: 0.0716\n",
            "Epoch 3/15\n",
            "19/19 [==============================] - 2s 102ms/step - loss: 5.3654 - accuracy: 0.0716\n",
            "Epoch 4/15\n",
            "19/19 [==============================] - 2s 105ms/step - loss: 5.2418 - accuracy: 0.0801\n",
            "Epoch 5/15\n",
            "19/19 [==============================] - 2s 105ms/step - loss: 5.0725 - accuracy: 0.0903\n",
            "Epoch 6/15\n",
            "19/19 [==============================] - 2s 107ms/step - loss: 4.7773 - accuracy: 0.1397\n",
            "Epoch 7/15\n",
            "19/19 [==============================] - 2s 113ms/step - loss: 4.4317 - accuracy: 0.1635\n",
            "Epoch 8/15\n",
            "19/19 [==============================] - 3s 164ms/step - loss: 4.0526 - accuracy: 0.2470\n",
            "Epoch 9/15\n",
            "19/19 [==============================] - 2s 107ms/step - loss: 3.6586 - accuracy: 0.3356\n",
            "Epoch 10/15\n",
            "19/19 [==============================] - 2s 106ms/step - loss: 3.2678 - accuracy: 0.4242\n",
            "Epoch 11/15\n",
            "19/19 [==============================] - 2s 107ms/step - loss: 2.8613 - accuracy: 0.5724\n",
            "Epoch 12/15\n",
            "19/19 [==============================] - 2s 108ms/step - loss: 2.4674 - accuracy: 0.6985\n",
            "Epoch 13/15\n",
            "19/19 [==============================] - 2s 128ms/step - loss: 2.1225 - accuracy: 0.7785\n",
            "Epoch 14/15\n",
            "19/19 [==============================] - 3s 161ms/step - loss: 1.7756 - accuracy: 0.8518\n",
            "Epoch 15/15\n",
            "19/19 [==============================] - 2s 110ms/step - loss: 1.4609 - accuracy: 0.9131\n",
            "1/1 [==============================] - 0s 393ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Generated Text:\n",
            "Shakespeare's plays are of canon of and 39 and plays written by the english poet playwright and actor william shakespeare the english number\n"
          ]
        }
      ],
      "source": [
        "#RNN IMPLEMENTATION\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Your input text (Shakespearean plays)\n",
        "input_text = \"\"\"\n",
        "Shakespeare's plays are a canon of approximately 39 dramatic works written by the English poet, playwright, and actor William Shakespeare. The exact number of plays as well as their classifications as tragedy, history, comedy, or otherwise is a matter of scholarly debate. Shakespeare's plays are widely regarded as among the greatest in the English language and are continually performed around the world. The plays have been translated into every major living language.\n",
        "\n",
        "Many of his plays appeared in print as a series of quartos, but approximately half of them remained unpublished until 1623, when the posthumous First Folio was published. The traditional division of his plays into tragedies, comedies, and histories follows the categories used in the First Folio. However, modern criticism has labelled some of these plays \"problem plays\" that elude easy categorisation, or perhaps purposely break generic conventions, and has introduced the term romances for what scholars believe to be his later comedies.\n",
        "\n",
        "When Shakespeare first arrived in London in the late 1580s or early 1590s, dramatists writing for London's new commercial playhouses (such as The Curtain) were combining two strands of dramatic tradition into a new and distinctively Elizabethan synthesis. Previously, the most common forms of popular English theatre were the Tudor morality plays. These plays, generally celebrating piety, use personified moral attributes to urge or instruct the protagonist to choose the virtuous life over Evil. The characters and plot situations are largely symbolic rather than realistic. As a child, Shakespeare would likely have seen this type of play (along with, perhaps, mystery plays and miracle plays).[1]\n",
        "\n",
        "The other strand of dramatic tradition was classical aesthetic theory. This theory was derived ultimately from Aristotle; in Renaissance England, however, the theory was better known through its Roman interpreters and practitioners. At the universities, plays were staged in a more academic form as Roman closet dramas. These plays, usually performed in Latin, adhered to classical ideas of unity and decorum, but they were also more static, valuing lengthy speeches over physical action. Shakespeare would have learned this theory at grammar school, where Plautus and especially Terence were key parts of the curriculum[2] and were taught in editions with lengthy theoretical introductions.[3]\n",
        "\n",
        "Theatre and stage setup\n",
        "Archaeological excavations on the foundations of the Rose and the Globe in the late twentieth century[4] suggested that all London English Renaissance theatres were built around similar general plans. Despite individual differences, the public theatres were three stories high and built around an open space at the center. Usually polygonal in plan to give an overall rounded effect, three levels of inward-facing galleries overlooked the open center into which jutted the stage—essentially a platform surrounded on three sides by the audience, only the rear being restricted for the entrances and exits of the actors and seating for the musicians. The upper level behind the stage could be used as a balcony, as in Romeo and Juliet, or as a position for a character to harangue a crowd, as in Julius Caesar.\n",
        "\n",
        "Usually built of timber, lath and plaster and with thatched roofs, the early theatres were vulnerable to fire, and gradually were replaced (when necessary) with stronger structures. When the Globe burned down in June 1613, it was rebuilt with a tile roof.\n",
        "\n",
        "A different model was developed with the Blackfriars Theatre, which came into regular use on a long term basis in 1599. The Blackfriars was small in comparison to the earlier theatres, and roofed rather than open to the sky; it resembled a modern theatre in ways that its predecessors did not.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([input_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequences of tokens\n",
        "input_sequences = []\n",
        "for line in input_text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create predictors and labels\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(SimpleRNN(150, return_sequences=True))\n",
        "model.add(SimpleRNN(150))  # You can add more layers if desired\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(predictors, label, epochs=15, verbose=1)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"Shakespeare's plays are\"\n",
        "next_words = 20\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list)[0]\n",
        "    predicted_index = np.argmax(predicted_probs)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(seed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION MECHANISM"
      ],
      "metadata": {
        "id": "aHeJ8HoIwYCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "# New English sentences\n",
        "english_sentences = ['start good morning end', 'start see you later end', 'start what are you doing end', 'start I am learning end', 'start have a nice day end']\n",
        "# Corresponding Turkish sentences\n",
        "turkish_sentences = ['start günaydın end', 'start görüşürüz end', 'start ne yapıyorsun end', 'start öğreniyorum end', 'start iyi günler end']\n",
        "\n",
        "# Tokenizing English sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "encoder_input_data = pad_sequences(english_sequences, padding='post')\n",
        "\n",
        "# Tokenizing Turkish sentences\n",
        "turkish_tokenizer = Tokenizer()\n",
        "turkish_tokenizer.fit_on_texts(turkish_sentences)\n",
        "turkish_sequences = turkish_tokenizer.texts_to_sequences(turkish_sentences)\n",
        "decoder_input_data = pad_sequences(turkish_sequences, padding='post')\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate\n",
        "from tensorflow.keras.layers import AdditiveAttention as Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "latent_dim = 512\n",
        "num_encoder_tokens = len(english_tokenizer.word_index) + 1\n",
        "num_decoder_tokens = len(turkish_tokenizer.word_index) + 1\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# Attention Layer\n",
        "attention = Attention()\n",
        "attention_output = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenating attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Preparing the target data for training\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "decoder_target_data = to_categorical(pad_sequences(turkish_sequences, padding='post'), num_decoder_tokens)\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "Eeyb7kxYwcC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ad9d8a-dd93-424d-8de9-59f043072985"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 8s 8s/step - loss: 2.3009 - accuracy: 0.1250 - val_loss: 2.2803 - val_accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 2.2481 - accuracy: 0.5000 - val_loss: 2.2633 - val_accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 2.1989 - accuracy: 0.5000 - val_loss: 2.2456 - val_accuracy: 0.5000\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 2.1403 - accuracy: 0.5000 - val_loss: 2.2269 - val_accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 2.0657 - accuracy: 0.5000 - val_loss: 2.2100 - val_accuracy: 0.5000\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 1.9712 - accuracy: 0.5000 - val_loss: 2.2065 - val_accuracy: 0.5000\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 1.8640 - accuracy: 0.5000 - val_loss: 2.2415 - val_accuracy: 0.5000\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 1.7734 - accuracy: 0.5000 - val_loss: 2.3100 - val_accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 1.6990 - accuracy: 0.5000 - val_loss: 2.3795 - val_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 1.6184 - accuracy: 0.5000 - val_loss: 2.4679 - val_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 1.5277 - accuracy: 0.6875 - val_loss: 2.5842 - val_accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 1.4279 - accuracy: 0.6875 - val_loss: 2.7496 - val_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 1.3215 - accuracy: 0.6875 - val_loss: 2.9773 - val_accuracy: 0.2500\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 1.2127 - accuracy: 0.6875 - val_loss: 3.3004 - val_accuracy: 0.5000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 1.1095 - accuracy: 0.6875 - val_loss: 3.7504 - val_accuracy: 0.2500\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 1.0822 - accuracy: 0.4375 - val_loss: 3.8137 - val_accuracy: 0.5000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 1.3518 - accuracy: 0.5000 - val_loss: 3.6163 - val_accuracy: 0.2500\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 1.2358 - accuracy: 0.6250 - val_loss: 3.1121 - val_accuracy: 0.5000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 1.1925 - accuracy: 0.5625 - val_loss: 4.3170 - val_accuracy: 0.2500\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 1.4333 - accuracy: 0.4375 - val_loss: 3.1701 - val_accuracy: 0.5000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.9825 - accuracy: 0.5000 - val_loss: 3.5734 - val_accuracy: 0.2500\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.8476 - accuracy: 0.6875 - val_loss: 3.8369 - val_accuracy: 0.2500\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.7853 - accuracy: 0.7500 - val_loss: 4.1288 - val_accuracy: 0.2500\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.7286 - accuracy: 0.9375 - val_loss: 4.3663 - val_accuracy: 0.2500\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.6755 - accuracy: 0.9375 - val_loss: 4.6915 - val_accuracy: 0.2500\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.6297 - accuracy: 0.8750 - val_loss: 4.7785 - val_accuracy: 0.5000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.6059 - accuracy: 0.9375 - val_loss: 5.4321 - val_accuracy: 0.2500\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.7075 - accuracy: 0.6250 - val_loss: 4.6831 - val_accuracy: 0.5000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 1.1032 - accuracy: 0.5625 - val_loss: 5.2926 - val_accuracy: 0.2500\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 1.1806 - accuracy: 0.5000 - val_loss: 3.9481 - val_accuracy: 0.5000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6097 - accuracy: 1.0000 - val_loss: 4.8081 - val_accuracy: 0.2500\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4984 - accuracy: 1.0000 - val_loss: 4.8935 - val_accuracy: 0.2500\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4597 - accuracy: 1.0000 - val_loss: 5.3271 - val_accuracy: 0.2500\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4294 - accuracy: 0.9375 - val_loss: 5.4211 - val_accuracy: 0.2500\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4059 - accuracy: 0.8750 - val_loss: 5.8920 - val_accuracy: 0.2500\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.3937 - accuracy: 0.8750 - val_loss: 5.6351 - val_accuracy: 0.2500\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4004 - accuracy: 0.8750 - val_loss: 6.5886 - val_accuracy: 0.2500\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4255 - accuracy: 0.8125 - val_loss: 5.2163 - val_accuracy: 0.2500\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4683 - accuracy: 0.8125 - val_loss: 6.8300 - val_accuracy: 0.2500\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4213 - accuracy: 0.8750 - val_loss: 5.2183 - val_accuracy: 0.2500\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4023 - accuracy: 0.8125 - val_loss: 6.5710 - val_accuracy: 0.2500\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.3398 - accuracy: 0.8750 - val_loss: 5.7220 - val_accuracy: 0.2500\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.3190 - accuracy: 0.8750 - val_loss: 6.6809 - val_accuracy: 0.2500\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.2980 - accuracy: 0.9375 - val_loss: 6.1153 - val_accuracy: 0.2500\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.2885 - accuracy: 0.8750 - val_loss: 6.9502 - val_accuracy: 0.2500\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.2845 - accuracy: 0.9375 - val_loss: 6.3005 - val_accuracy: 0.2500\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.2843 - accuracy: 0.8750 - val_loss: 7.2330 - val_accuracy: 0.2500\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.2929 - accuracy: 0.9375 - val_loss: 6.2606 - val_accuracy: 0.2500\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2882 - accuracy: 0.9375 - val_loss: 7.5115 - val_accuracy: 0.2500\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.2891 - accuracy: 0.8125 - val_loss: 6.0720 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c5a3d2da650>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}